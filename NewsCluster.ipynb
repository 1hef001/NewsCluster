{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsCluster.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvdNDexnnZiw",
        "colab_type": "code",
        "outputId": "4e4561b3-1b08-40be-d48a-fd4f5c10ad47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "!pip install seaborn\n",
        "!pip install newspaper3k\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.0.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.18.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn) (1.12.0)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (5.2.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.2.2)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.4.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_hLHhvB0bnL",
        "colab_type": "text"
      },
      "source": [
        "The next two lines are required to load files from your Google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsuL2iCp0gkE",
        "colab_type": "code",
        "outputId": "fa0737e2-6e86-467b-9bdd-21ce67c24a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcV8YJ5G1fKJ",
        "colab_type": "text"
      },
      "source": [
        "# SCRAPER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTwYibwA1iHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from newspaper import Article\n",
        "from newspaper import ArticleException\n",
        "import newspaper\n",
        "# from progress.bar import IncrementalBar\n",
        "import time\n",
        "import string\n",
        "\n",
        "\n",
        "def scrape_news_links(url):\n",
        "    '''\n",
        "        Scrapes links : not only google but any online vendor.\n",
        "        set url while calling the function\n",
        "    '''\n",
        "    print('Scraping links')\n",
        "    paper = newspaper.build(url, memoize_articles=False)\n",
        "    links = []\n",
        "    # bar = IncrementalBar('Scraping Links', max=len(paper.articles), suffix='%(percent)d%%')\n",
        "    for article in paper.articles:\n",
        "        links.append(article.url)\n",
        "        # bar.next()\n",
        "        time.sleep(0.1)\n",
        "    # bar.finish()\n",
        "    \n",
        "    # print(links)\n",
        "    return links\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "        To clean text\n",
        "    '''\n",
        "    print('cleaning_text')\n",
        "    # text = text.strip()\n",
        "    # text = text.lower()\n",
        "    # for punct in string.punctuation:\n",
        "    #     text = text.replace(punct, '')\n",
        "    text = text.lower()\n",
        "    strin = text.split('\\n')\n",
        "    text = \" \".join(strin)\n",
        "    # text.replace('\\\\', '')\n",
        "    exclude = set(string.punctuation)\n",
        "    text = ''.join(ch for ch in text if ch not in exclude)\n",
        "    return text\n",
        "\n",
        "def get_content(links):\n",
        "    '''\n",
        "        get headlines and news content\n",
        "    '''\n",
        "    print('getting content')\n",
        "    content = []\n",
        "    # next_bar = IncrementalBar('Getting Content', max=)\n",
        "    # bar = IncrementalBar('Getting content & Cleaning text', max=len(links), suffix='%(percent)d%%' )\n",
        "    for url in links:\n",
        "        try:\n",
        "            article = Article(url, language='en')\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            title = clean_text(article.title)\n",
        "            news = clean_text(article.text)\n",
        "            if title != None:\n",
        "                if news != None: \n",
        "                    if news != ' ': \n",
        "                        if news != '':      # for sites which news content cannot be scraped\n",
        "                            content.append([title, news])\n",
        "            # bar.next()\n",
        "    \n",
        "        except ArticleException as ae:\n",
        "            # if 'Article \\'download()\\' failed' in ae:\n",
        "            continue\n",
        "    \n",
        "    # bar.finish()\n",
        "    return content\n",
        "    \n",
        "\n",
        "def scraper(link='https://timesofindia.indiatimes.com/'):\n",
        "    '''\n",
        "        aggregator function\n",
        "    '''\n",
        "    # print('scraper_main')5\n",
        "    return get_content(scrape_news_links(link))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "    # links = scrape_google_links()\n",
        "    # print(get_content(links[:15]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2PD4dSo1pwm",
        "colab_type": "text"
      },
      "source": [
        "# DF AND CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3wWToml1t-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "LINKS = ['https://timesofindia.indiatimes.com/',\n",
        "         'https://www.thehindu.com/',\n",
        "         'https://www.bbc.com/news',\n",
        "         'https://www.theguardian.co.uk/',\n",
        "         'https://www.hindustantimes.com/',\n",
        "         'https://indianexpress.com/',\n",
        "         'https://www.dailypioneer.com/'\n",
        "         'https://www.deccanherald.com/',\n",
        "         'https://www.telegraphindia.com/',\n",
        "         'https://www.dnaindia.com/',\n",
        "         'https://www.deccanchronicle.com/',\n",
        "         'https://www.asianage.com/',\n",
        "         'https://economictimes.indiatimes.com/',\n",
        "         'https://www.tribuneindia.com/']\n",
        "\n",
        "def create_df(content_list):\n",
        "    '''\n",
        "        To write the data to csv file\n",
        "        takes a list of list where the inner list contains ['headline', 'news']\n",
        "    '''\n",
        "    title = []\n",
        "    news = []\n",
        "    print('creating_dataFrame')\n",
        "\n",
        "    for content in content_list:\n",
        "        title.append(content[0])\n",
        "        news.append(content[1])\n",
        "        # keywords.append(content[2])\n",
        "\n",
        "    data = {'Title' : title, 'News' : news}\n",
        "    df = pd.DataFrame(data, columns=['Title', 'News'])\n",
        "    return df\n",
        "\n",
        "\n",
        "def df_to_csv(df, filename='NewsCluster.csv'):\n",
        "    '''\n",
        "        writes dataframe to csv\n",
        "    '''\n",
        "    print('writing_to_csv')\n",
        "    df.to_csv('/content/drive/My Drive/data/' + filename)\n",
        "\n",
        "\n",
        "def create_csv():\n",
        "    '''\n",
        "        aggregator function of this module\n",
        "    '''\n",
        "    print('create_csv_main')\n",
        "    content_list = []\n",
        "    for link in LINKS:\n",
        "        content_list.append(scraper(link))\n",
        "\n",
        "    content_lst = []\n",
        "    for content in content_list:\n",
        "        for cont in content:\n",
        "            content_lst.append(cont)\n",
        "    # content_lst = scraper()\n",
        "    # print(content_lst)\n",
        "    try:\n",
        "        num = int(input('Enter the number of articles to be stored : '))\n",
        "        if num < 15:\n",
        "            raise ValueError('Provide a larger number for dataset')\n",
        "        df_to_csv(create_df(content_lst[:num]))\n",
        "    except ValueError as ve:\n",
        "        df_to_csv(create_df(content_lst))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3w7yuHE16T8",
        "colab_type": "text"
      },
      "source": [
        "# CONVERT TO DB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd2-vaUw19yQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sqlite3\n",
        "from sqlite3 import IntegrityError\n",
        "import csv\n",
        "\n",
        "def insert_to_db(tup):\n",
        "    with sqlite3.connect('/content/drive/My Drive/data/NEWS.DB') as con:\n",
        "        cur = con.cursor()\n",
        "        cur.execute(\"INSERT INTO content (headlines, news) VALUES(?, ?);\", tup)\n",
        "        con.commit()\n",
        "\n",
        "def to_database():\n",
        "    '''\n",
        "        converts csv to db\n",
        "    '''\n",
        "    with sqlite3.connect('/content/drive/My Drive/data/NEWS.DB') as con:\n",
        "        cur = con.cursor()\n",
        "        cur.execute('CREATE TABLE IF NOT EXISTS content(headlines TEXT, news TEXT PRIMARY KEY);')\n",
        "        with open('/content/drive/My Drive/data/NewsCluster.csv', encoding='utf-8') as fin:\n",
        "            dr = csv.DictReader(fin)\n",
        "            for i in dr:\n",
        "                try:\n",
        "                    tup = (i['Title'], i['News'])\n",
        "                    insert_to_db(tup)\n",
        "                except IntegrityError as ie:\n",
        "                    # if 'unique constraint' in ie:\n",
        "                    continue\n",
        "\n",
        "            # to_db = [(i['Title'], i['News']) for i in dr]\n",
        "        \n",
        "        # cur.executemany(\"INSERT INTO content (headlines, news) VALUES(?, ?);\", to_db)\n",
        "        con.commit()\n",
        "    con.close()\n",
        "\n",
        "def print_db():\n",
        "    '''\n",
        "        prints database\n",
        "        used for reference and verification\n",
        "    '''\n",
        "    with sqlite3.connect(\"/content/drive/My Drive/data/NEWS.DB\") as con:\n",
        "        cur = con.cursor()\n",
        "        cur.execute('SELECT * FROM content')\n",
        "        return cur.fetchall()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "    '''\n",
        "    execute either of the functions to update database or displahy the content\n",
        "    '''\n",
        "    # to_database()\n",
        "    # print(print_db()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opnElspN2Elx",
        "colab_type": "text"
      },
      "source": [
        "# CALL SCRAPER, CREATE CSV and DB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZgiog3U2JZ0",
        "colab_type": "code",
        "outputId": "41991eec-90d0-4710-d9cc-19736045849d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "create_csv()\n",
        "to_database()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "create_csv_main\n",
            "Scraping links\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-23023a878e1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mto_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-f0a53a5a8b7d>\u001b[0m in \u001b[0;36mcreate_csv\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcontent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLINKS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mcontent_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mcontent_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-606a112e99bc>\u001b[0m in \u001b[0;36mscraper\u001b[0;34m(link)\u001b[0m\n\u001b[1;32m     77\u001b[0m     '''\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# print('scraper_main')5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_news_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-606a112e99bc>\u001b[0m in \u001b[0;36mscrape_news_links\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     13\u001b[0m     '''\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scraping links'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mpaper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewspaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoize_articles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# bar = IncrementalBar('Scraping Links', max=len(paper.articles), suffix='%(percent)d%%')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/newspaper/api.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(url, dry, config, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/newspaper/source.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# mthread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/newspaper/source.py\u001b[0m in \u001b[0;36mdownload_categories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m    185\u001b[0m         \u001b[0mcategory_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mrequests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultithread_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/newspaper/network.py\u001b[0m in \u001b[0;36mmultithread_request\u001b[0;34m(urls, config)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm_requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/newspaper/mthreading.py\u001b[0m in \u001b[0;36mwait_completion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_tasks_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfinished_tasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_tasks_done\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj_YO_3X1nGl",
        "colab_type": "text"
      },
      "source": [
        "# CHECK CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9jN8x3Q0kjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def print_csv(filename):\n",
        "  with open('/content/drive/My Drive/data/'+filename) as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      for row in csv_reader:\n",
        "        print(row)\n",
        "\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "    print_csv(\"NewsCluster.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln3KJWAZ1Vnz",
        "colab_type": "text"
      },
      "source": [
        "# CLUSTERING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tPzu9P7jlv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Wrapper for offline clustering methods that do not take into\n",
        "account temporal aspects of data and online clustering methods\n",
        "that update and/or predict new data as it comes in. Framework\n",
        "supports custom text representations (e.g. Continuous Bag of\n",
        "Words) but will default to tfidf if none are provided.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import MDS\n",
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "# from hdbscan import HDBSCAN\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from scipy.sparse import issparse, vstack\n",
        "from sklearn.cluster import *\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk_stopwords = stopwords.words('english')\n",
        "\n",
        "\n",
        "class Cluster:\n",
        "    \"\"\" Clustering methods for text. Be cautious of datasize; in cases\n",
        "    of large data, KMeans may be the only efficient choice.\n",
        "\n",
        "    Accepts custom matrices\n",
        "\n",
        "    Full analysis of methods can be found at:\n",
        "    http://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html\n",
        "\n",
        "    Usage:\n",
        "        >> with open('../data/cleaned_text.txt', 'r', encoding='utf8') as f:\n",
        "               text = f.readlines()\n",
        "        >> clustering = Cluster(text)\n",
        "        >> results = clustering('hdbscan', matrix=None, reduce_dim=None,\n",
        "                                visualize=True, top_terms=False,\n",
        "                                min_cluster_size=10)\n",
        "        >> print(results)\n",
        "    \"\"\"\n",
        "    def __init__(self, text):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text: strings to be clustered (list of strings)\n",
        "        \"\"\"\n",
        "        self.text = list(set(text))\n",
        "\n",
        "    def __call__(self, method, vectorizer=None,\n",
        "                         reduce_dim=None, viz=False,\n",
        "                         *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            method: algorithm to use to cluster data (str)\n",
        "            vectorizer: initialized method to convert text to np array;\n",
        "                        assumes __call__ vectorizes the text (Class, optional)\n",
        "            reduce_dim: reduce dim of representation matrix (int, optional)\n",
        "            visualize: visualize clusters in 3D (bool, optional)\n",
        "            *args, **kwargs: see specified method function\n",
        "        \"\"\"\n",
        "\n",
        "        # Make sure method is valid\n",
        "        assert method in ['hdbscan', 'dbscan', 'spectral', 'kmeans',\n",
        "                          'minikmeans', 'affinity_prop', 'agglomerative',\n",
        "                          'mean_shift', 'birch'], 'Invalid method chosen.'\n",
        "\n",
        "        if not hasattr(self, 'vectorizer'):\n",
        "            if vectorizer is None:\n",
        "                self._init_tfidf()\n",
        "            else:\n",
        "                self.vectorizer = vectorizer\n",
        "                self.matrix = self.vectorizer(self.text)\n",
        "\n",
        "        # Reduce dimensionality using latent semantic analysis (makes faster)\n",
        "        if reduce_dim is not None:\n",
        "            self.matrix = self._pca(reduce_dim, self.matrix)\n",
        "\n",
        "        # Cache current method\n",
        "        method = eval('self.' + method)\n",
        "        self.algorithm = method(*args, **kwargs)\n",
        "        self.results = self._organize(self.algorithm.labels_)\n",
        "\n",
        "        # For plotting\n",
        "        self.viz_matrix = self.matrix\n",
        "\n",
        "        # Visualize clustering outputs if applicable\n",
        "        if viz:\n",
        "            # _ = self.viz2D()\n",
        "            _ = self.viz3D()\n",
        "            _ = self.top_terms()\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    # def hdbscan(self, min_cluster_size=10, prediction_data=False):\n",
        "    #     \"\"\" DBSCAN but allows for varying density clusters and no longer\n",
        "    #     requires epsilon parameter, which is difficult to tune.\n",
        "    #     http://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html\n",
        "    #     Scales slightly worse than DBSCAN, but with a more intuitive parameter.\n",
        "    #     \"\"\"\n",
        "    #     hdbscan = HDBSCAN(min_cluster_size=min_cluster_size,\n",
        "    #                         prediction_data=prediction_data)\n",
        "    #     if prediction_data:\n",
        "    #         return hdbscan.fit(self._safe_dense(self.matrix))\n",
        "    #     else:\n",
        "    #         return hdbscan.fit(self.matrix)\n",
        "\n",
        "    def dbscan(self, eps=0.50):\n",
        "        \"\"\" Density-based algorithm that clusters points in dense areas and\n",
        "        distances points in sparse areas. Stable, semi-fast, non-global.\n",
        "        Scales very well with n_samples, decently with n_clusters (not tunable)\n",
        "        \"\"\"\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=3)\n",
        "        return dbscan.fit(self.matrix)\n",
        "\n",
        "    def kmeans(self, n_clusters=10, n_init=5):\n",
        "        km = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, n_init=n_init, verbose=0, random_state=3425)\n",
        "        return km.fit(self.matrix)\n",
        "\n",
        "    def minikmeans(self, n_clusters=10, n_init=5, batch_size=5000):\n",
        "        \"\"\" Partition dataset into n_cluster global chunks by minimizing\n",
        "        intra-partition distances. Expect quick results, but with noise.\n",
        "        Scales exceptionally well with n_samples, decently with n_clusters.\n",
        "        \"\"\"\n",
        "        kmeans = MiniBatchKMeans(n_clusters=n_clusters,\n",
        "                                 init='k-means++',\n",
        "                                 n_init=n_init,\n",
        "                                 batch_size=batch_size)\n",
        "        return kmeans.fit(self.matrix)\n",
        "\n",
        "    def birch(self, n_clusters=10):\n",
        "        \"\"\" Partitions dataset into n_cluster global chunks by repeatedly\n",
        "        merging subclusters of a CF tree. Birch does not scale very well to high\n",
        "        dimensional data. If many subclusters are desired, set n_clusters=None.\n",
        "        Scales well with n_samples, well with n_clusters.\n",
        "        \"\"\"\n",
        "        birch = Birch(n_clusters=n_clusters)\n",
        "        return birch.fit(self.matrix)\n",
        "\n",
        "    def agglomerative(self, n_clusters=10, linkage='ward'):\n",
        "        \"\"\" Iteratively clusters dataset semi-globally by starting with each\n",
        "        point in its own cluster and then using some criterion to choose another\n",
        "        cluster to merge that cluster with another cluster.\n",
        "        Scales well with n_samples, decently with n_clusters.\n",
        "        \"\"\"\n",
        "        agglomerative = AgglomerativeClustering(n_clusters=n_clusters,\n",
        "                                                linkage=linkage)\n",
        "        return agglomerative.fit(self._safe_dense(self.matrix))\n",
        "\n",
        "    def spectral(self, n_clusters=5):\n",
        "        \"\"\" Partitions dataset semi-globally by inducing a graph based on the\n",
        "        distances between points and trying to learn a manifold, and then\n",
        "        running a standard clustering algorithm (e.g. KMeans) on this manifold.\n",
        "        Scales decently with n_samples, poorly with n_clusters.\n",
        "        \"\"\"\n",
        "        spectral = SpectralClustering(n_clusters=n_clusters)\n",
        "        return spectral.fit(self.matrix)\n",
        "\n",
        "    def affinity_prop(self, damping=0.50):\n",
        "        \"\"\" Partitions dataset globally using a graph based approach to let\n",
        "        points ‘vote’ on their preferred ‘exemplar’.\n",
        "        Does not scale well with n_samples. Not recommended to use with text.\n",
        "        \"\"\"\n",
        "        affinity_prop = AffinityPropagation(damping=damping)\n",
        "        return affinity_prop.fit(self._safe_dense(self.matrix))\n",
        "\n",
        "    def mean_shift(self, cluster_all=False):\n",
        "        \"\"\" Centroid-based, global method that assumes there exists some\n",
        "        probability density function from which the data is drawn, and tries to\n",
        "        place centroids of clusters  at the maxima of that density function.\n",
        "        Unstable, but conservative.\n",
        "        Does not scale well with n_samples. Not recommended to use with text.\n",
        "        \"\"\"\n",
        "        mean_shift = MeanShift(cluster_all=False)\n",
        "        return mean_shift.fit(self._safe_dense(self.matrix))\n",
        "\n",
        "    def _init_tfidf(self, max_features=30000, analyzer='word',\n",
        "                    stopwords=nltk_stopwords, token_pattern=r\"(?u)\\b\\w+\\b\"):\n",
        "        \"\"\" Default representation for data is sparse tfidf vectors\n",
        "\n",
        "        Args:\n",
        "            max_features: top N vocabulary to consider (int)\n",
        "            analyzer: 'word' or 'char', level at which to segment text (str)\n",
        "            stopwords: words to remove from consideration, default nltk (list)\n",
        "        \"\"\"\n",
        "        # Initialize and fit tfidf vectors\n",
        "        self.vectorizer = TfidfVectorizer(max_features=max_features,\n",
        "                                             stop_words=stopwords,\n",
        "                                             analyzer=analyzer,\n",
        "                                             token_pattern=token_pattern)\n",
        "        self.matrix = self.vectorizer.fit_transform(self.text)\n",
        "\n",
        "        # Get top max_features vocabulary\n",
        "        self.terms = self.vectorizer.get_feature_names()\n",
        "\n",
        "        # For letting user know if tfidf has been initialized\n",
        "        self.using_tfidf = True\n",
        "\n",
        "    def viz2D(self, matrix=None,\n",
        "                plot_kwds={'alpha':0.30, 's':40, 'linewidths':0}):\n",
        "        \"\"\" Visualize clusters in 2D \"\"\"\n",
        "        # Run PCA over the data so we can plot\n",
        "        # matrix2D = self._pca(n=2, matrix=self.viz_matrix)\n",
        "\n",
        "        # # Get labels\n",
        "        # labels = np.unique(self.results['labels'])\n",
        "\n",
        "        # # Assign a color to each label\n",
        "        # palette = sns.color_palette('deep', max(labels)+1)\n",
        "        # colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
        "\n",
        "        # # Plot the data\n",
        "        # plt.close()\n",
        "        # fig = plt.figure(figsize=(10,6))\n",
        "        # plt.scatter(matrix2D.T[0],\n",
        "        #             matrix2D.T[1],\n",
        "        #             c=colors,\n",
        "        #             **plot_kwds\n",
        "        #             )\n",
        "        # frame = plt.gca()\n",
        "\n",
        "        # # Turn off axes, since they are arbitrary\n",
        "        # frame.axes.get_xaxis().set_visible(False)\n",
        "        # frame.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "        # # Add a title\n",
        "        # alg_name = str(self.algorithm.__class__.__name__)\n",
        "        # plt.title('{0} clusters found by {1}'.format(len(labels),\n",
        "        #                                              alg_name),\n",
        "        #           fontsize=20)\n",
        "        # plt.tight_layout()\n",
        "        # plt.show()\n",
        "        # return fig\n",
        "\n",
        "        # Run PCA over the data\n",
        "        matrix3D = self._pca(n=2, matrix=self.viz_matrix)\n",
        "\n",
        "        # Extract labels from results\n",
        "        labels = self.results['labels']\n",
        "\n",
        "        # Assign colors\n",
        "        palette = sns.color_palette('deep', int(max(labels)+1))\n",
        "        colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
        "\n",
        "        # Plot the data\n",
        "        plt.close()\n",
        "        fig = plt.figure(figsize=(10,6))\n",
        "        # ax = plt.axes(projection='3d')\n",
        "        plt.scatter(matrix3D.T[0],\n",
        "                   matrix3D.T[1],\n",
        "                  #  matrix3D.T[2],\n",
        "                   c=colors)\n",
        "\n",
        "        # Add a title\n",
        "        alg_name = str(self.algorithm.__class__.__name__)\n",
        "        plt.title('{0} Clusters | {1} Items | {2}'.format(len(set(labels)),\n",
        "                                                            matrix3D.shape[0],\n",
        "                                                            alg_name),\n",
        "                  fontsize=20)\n",
        "\n",
        "        # Turn off arbitrary axis tick labels\n",
        "        # plt.tick_params(axis='both', left=False, top=False, right=False,\n",
        "        #                 bottom=False, labelleft=False, labeltop=False,\n",
        "        #                 labelright=False, labelbottom=False)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        return fig\n",
        "\n",
        "\n",
        "\n",
        "    def viz3D(self, matrix=None):\n",
        "        \"\"\" Visualize clusters in 3D \"\"\"\n",
        "        # Run PCA over the data\n",
        "        matrix3D = self._pca(n=3, matrix=self.viz_matrix)\n",
        "\n",
        "        # Extract labels from results\n",
        "        labels = self.results['labels']\n",
        "\n",
        "        # Assign colors\n",
        "        palette = sns.color_palette('deep', int(max(labels)+1))\n",
        "        colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
        "\n",
        "        # Plot the data\n",
        "        plt.close()\n",
        "        fig = plt.figure(figsize=(10,6))\n",
        "        ax = plt.axes(projection='3d')\n",
        "        ax.scatter(matrix3D.T[0],\n",
        "                   matrix3D.T[1],\n",
        "                   matrix3D.T[2],\n",
        "                   c=colors)\n",
        "\n",
        "        # Add a title\n",
        "        alg_name = str(self.algorithm.__class__.__name__)\n",
        "        plt.title('{0} Clusters | {1} Items | {2}'.format(len(set(labels)),\n",
        "                                                            matrix3D.shape[0],\n",
        "                                                            alg_name),\n",
        "                  fontsize=20)\n",
        "\n",
        "        # Turn off arbitrary axis tick labels\n",
        "        plt.tick_params(axis='both', left=False, top=False, right=False,\n",
        "                        bottom=False, labelleft=False, labeltop=False,\n",
        "                        labelright=False, labelbottom=False)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        return fig\n",
        "\n",
        "    def top_terms(self, topx=10):\n",
        "        \"\"\" Print out top terms per cluster. \"\"\"\n",
        "        if self.using_tfidf != True:\n",
        "            print('For use with non-tfidf vectorizers,try sklearn NearestNeighbors\\\n",
        "            (although NN performs poorly with high dimensional inputs.')\n",
        "            return None\n",
        "\n",
        "        # Get labels, sort text IDs by cluster\n",
        "        labels = self.results['labels']\n",
        "        cluster_idx = {clust_id: np.where(labels == clust_id)[0]\n",
        "                       for clust_id in set(labels)}\n",
        "\n",
        "        # Get centers, stack into array\n",
        "        centroids = np.vstack([self.viz_matrix[indexes].mean(axis=0)\n",
        "                                for key, indexes in cluster_idx.items()])\n",
        "\n",
        "        # Compute closeness of each term representation to each centroid\n",
        "        order_centroids = np.array(centroids).argsort()[:, ::-1]\n",
        "\n",
        "        # Organize terms into a dictionary\n",
        "        cluster_terms = {clust_id: [self.terms[ind]\n",
        "                                    for ind in order_centroids[idx, :topx]]\n",
        "                        for idx, clust_id in enumerate(cluster_idx.keys())}\n",
        "\n",
        "        # Print results\n",
        "        print(\"Top terms per cluster:\")\n",
        "        for clust_id, terms in cluster_terms.items():\n",
        "            words = ' | '.join(terms)\n",
        "            print(\"Cluster {0} ({1} items): {2}\".format(clust_id,\n",
        "                                                        len(cluster_idx[clust_id]),\n",
        "                                                        words))\n",
        "\n",
        "        return cluster_terms\n",
        "\n",
        "    def item_counts(self):\n",
        "        \"\"\" Print number of counts in each cluster \"\"\"\n",
        "        for key, vals in self.results.items():\n",
        "            if key == 'labels':\n",
        "                continue\n",
        "            print('Cluster {0}: {1} items'.format(key, len(vals)))\n",
        "\n",
        "    def _organize(self, labels):\n",
        "        \"\"\" Organize text from clusters into a dictionary \"\"\"\n",
        "        # Organize text into respective clusters\n",
        "        cluster_idx = {clust_id: np.where(labels == clust_id)[0]\n",
        "                       for clust_id in set(labels)}\n",
        "\n",
        "        # Put results in a dictionary; key is cluster idx values are text\n",
        "        results = {clust_id: [self.text[idx] for idx in cluster_idx[clust_id]]\n",
        "                    for clust_id in cluster_idx.keys()}\n",
        "        results['labels'] = list(labels)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _pca(self, n, matrix):\n",
        "        \"\"\" Perform PCA on the data \"\"\"\n",
        "        return TruncatedSVD(n_components=n).fit_transform(matrix)\n",
        "\n",
        "    def _safe_dense(self, matrix):\n",
        "        \"\"\" Some algorithms don't accept sparse input; for these, make\n",
        "        sure the input matrix is dense. \"\"\"\n",
        "        if issparse(matrix):\n",
        "            return matrix.todense()\n",
        "        else:\n",
        "            return matrix\n",
        "\n",
        "\n",
        "class OnlineCluster(Cluster):\n",
        "    \"\"\" Online (stream) clustering of textual data. Check each method\n",
        "    to determine if the model is updating or ad-hoc predicting. These are not\n",
        "    'true' online methods as they preserve all seen data, as opposed to letting\n",
        "    data points and clusters fade, merge, etc. over time.\n",
        "\n",
        "    Usage:\n",
        "        To initialize:\n",
        "        >> with open('../data/cleaned_text.txt', 'r', encoding='utf8') as f:\n",
        "               text = f.readlines()\n",
        "        >> online = OnlineCluster(method='kmeans', text, visualize=True)\n",
        "\n",
        "        To predict and update parameters if applicable:\n",
        "        >> new_text = text[-10:]\n",
        "        >> online.predict(new_text)\n",
        "    \"\"\"\n",
        "    def __init__(self, text, method, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text: strings to be clustered (list of strings)\n",
        "            method: algorithm to use to cluster (string)\n",
        "            *args, **kwargs (optional):\n",
        "                vectorizer: text representation. Defaults tfidf (array, optional)\n",
        "                reduce_dim: reduce dim of representation matrix (int, optional)\n",
        "                visualize: visualize clusters in 3D (bool, optional)\n",
        "        \"\"\"\n",
        "        # Only accept valid arguments\n",
        "        assert method in ['kmeans', 'birch', 'hdbscan',\n",
        "                          'dbscan', 'mean_shift'], \\\n",
        "                'Method incompatible with online clustering.'\n",
        "\n",
        "        # Initialize inherited class\n",
        "        super().__init__(text)\n",
        "\n",
        "        # Get initial results\n",
        "        self.results = self.__call__(method=method, *args,**kwargs)\n",
        "\n",
        "        # Save args, set method\n",
        "        self.__dict__.update(locals())\n",
        "        self.method = eval('self._' + method)\n",
        "\n",
        "    def predict(self, new_text):\n",
        "        \"\"\" 'Predict' a new example based on cluster centroids and update params\n",
        "        if applicable (kmeans, birch). If a custom (non-tfidf) text representation\n",
        "        is being used, class assumes new_text is already in vectorized form.\n",
        "\n",
        "        Args:\n",
        "            new_text: list of strings to predict\n",
        "        \"\"\"\n",
        "        # Predict\n",
        "        assert type(new_text) == list, 'Input should be list of strings.'\n",
        "        self.text = list(set(self.text + new_text))\n",
        "        new_matrix = self._transform(new_text)\n",
        "        output_labels = self.method(new_matrix)\n",
        "\n",
        "        # Update attribute for results, plotting\n",
        "        self._update_results(output_labels)\n",
        "        self.viz_matrix = vstack([self.viz_matrix, new_matrix])\n",
        "        return output_labels\n",
        "\n",
        "    def _kmeans(self, new_matrix):\n",
        "        \"\"\" Updates parameters and predicts \"\"\"\n",
        "        self.algorithm = self.algorithm.partial_fit(new_matrix)\n",
        "        return self.algorithm.predict(new_matrix)\n",
        "\n",
        "    def _birch(self, new_matrix):\n",
        "        \"\"\" Updates parameters and predicts \"\"\"\n",
        "        self.algorithm = self.algorithm.partial_fit(new_matrix)\n",
        "        return self.algorithm.predict(new_matrix)\n",
        "\n",
        "    def _hdbscan(self, new_matrix):\n",
        "        \"\"\" Prediction only, HDBSCAN requires training to be done on dense\n",
        "        matrices for prediction to work properly. This makes training\n",
        "        inefficient, though. \"\"\"\n",
        "        try:\n",
        "            labels, _ = approximate_predict(self.algorithm,\n",
        "                                            self._safe_dense(new_matrix))\n",
        "        except AttributeError:\n",
        "            try:\n",
        "                self.algorithm.generate_prediction_data()\n",
        "                labels, _ = approximate_predict(self.algorithm,\n",
        "                                                self._safe_dense(new_matrix))\n",
        "            except ValueError:\n",
        "                print('Must (inefficiently) re-train with prediction_data=True')\n",
        "        return labels\n",
        "\n",
        "    def _dbscan(self, new_matrix):\n",
        "        \"\"\" Prediction only \"\"\"\n",
        "        # Extract labels\n",
        "        labels = self.algorithm.labels_\n",
        "\n",
        "        # Result is noise by default\n",
        "        output = np.ones(shape=new_matrix.shape[0], dtype=int)*-1\n",
        "\n",
        "        # Iterate all input samples for a label\n",
        "        for idx, row in enumerate(new_matrix):\n",
        "\n",
        "            # Find a core sample closer than EPS\n",
        "            for i, row in enumerate(self.algorithm.components_):\n",
        "\n",
        "                # If it's below the threshold of the dbscan model\n",
        "                if cosine(row, x_core) < self.algorithm.eps:\n",
        "\n",
        "                    # Assign label of x_core to the input sample\n",
        "                    output[idx] = labels[self.algorithm.core_sample_indices_[i]]\n",
        "                    break\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _mean_shift(self, new_matrix):\n",
        "        \"\"\" Prediction only, not efficient \"\"\"\n",
        "        return self.algorithm.predict(new_matrix)\n",
        "\n",
        "    def _transform(self, new_text):\n",
        "        \"\"\" Transform text to tfidf representation. Assumes already vectorized\n",
        "        if tfidf matrix has not been initialized. \"\"\"\n",
        "        if self.using_tfidf:\n",
        "            return self.vectorizer.transform(new_text)\n",
        "        else:\n",
        "            return self.vectorizer(new_text)\n",
        "        return new_matrix\n",
        "\n",
        "    def _update_results(self, labels):\n",
        "        \"\"\" Update running dictionary \"\"\"\n",
        "        new_results = self._organize(labels)\n",
        "        for key in self.results.keys():\n",
        "            try:\n",
        "                self.results[key] += new_results[key]\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yqf5RxeiIVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "cluster_dict = {2:'dbscan', 3:'spectral', 4:'kmeans', 5:'affinity_prop', 6:'agglomerative', 7:'mean_shift', 8:'birch'}\n",
        "\n",
        "def clean(text):\n",
        "    '''\n",
        "        Clean text before running clusterer\n",
        "    '''\n",
        "    text = text.strip()\n",
        "    text = text.lower()\n",
        "    for punct in string.punctuation:\n",
        "        text = text.replace(punct, ' ')\n",
        "    lst = text.split()\n",
        "    text = \" \".join(lst)\n",
        "    for t in text:\n",
        "        if t not in string.printable:\n",
        "            text = text.replace(t, '')\n",
        "    return text\n",
        "\n",
        "def clust():\n",
        "    df = pd.read_csv('/content/drive/My Drive/data/NewsCluster.csv')\n",
        "    data = df[\"Title\"].tolist()\n",
        "\n",
        "    data = [clean(dt) for dt in data ]\n",
        "\n",
        "    # for dt in data:\n",
        "    #     data[data.index(dt)] = clean(dt)\n",
        "\n",
        "    data = pd.DataFrame(data, columns=[\"text\"])\n",
        "    data['text'].dropna(inplace=True)\n",
        "\n",
        "    # %matplotlib inline\n",
        "\n",
        "\n",
        "    clustering = Cluster(data.text)\n",
        "    # results = clustering(method='dbscan', vectorizer=None, \n",
        "    #                     reduce_dim=None, viz=True, eps=0.9)\n",
        "    \n",
        "    results = clustering(method='kmeans', vectorizer=None, \n",
        "                        reduce_dim=None, viz=True, n_clusters=12)\n",
        "    \n",
        "    # results = clustering(method='birch', vectorizer=None, \n",
        "    #                     reduce_dim=None, viz=True, n_clusters=12)\n",
        "    \n",
        "    # results = clustering(method='agglomerative', vectorizer=None, \n",
        "    #                     reduce_dim=None, viz=True, n_clusters=12)\n",
        "    \n",
        "    # results = clustering(method='spectral', vectorizer=None, \n",
        "    #                     reduce_dim=None, viz=True, n_clusters=12)\n",
        "    \n",
        "    # results = clustering(method='affinity_prop', vectorizer=None, \n",
        "    #                     reduce_dim=None, viz=True, damping=0.5)\n",
        "    \n",
        "    results = clustering(method='minikmeans', vectorizer=None, \n",
        "                        reduce_dim=None, viz=True, n_clusters=12)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # clustering = Cluster(data.text)\n",
        "    # for i in range(2,9):\n",
        "    #     print(cluster_dict[i])\n",
        "    #     if i == 4:\n",
        "    #         result = clustering(cluster_dict[i])\n",
        "    #     else:\n",
        "    #         result = clustering(cluster_dict[i])\n",
        "        \n",
        "    #     print(result)\n",
        "\n",
        "clust()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}